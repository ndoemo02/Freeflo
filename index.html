<!DOCTYPE html>
<html lang="pl">
<head>
  <meta charset="UTF-8" />
  <title>FreeFlow Voice</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    body { font-family: system-ui, sans-serif; background:#111; color:#fff; text-align:center; margin:0; padding:32px; }
    #micBtn { width:120px; height:120px; border-radius:50%; background:#222; border:none; cursor:pointer; margin:20px auto; display:block; font-size:48px; }
    #micBtn.pulse { animation:pulse 1.5s infinite; }
    @keyframes pulse {
      0% { box-shadow:0 0 0 0 rgba(0,255,0,.7); }
      70% { box-shadow:0 0 0 15px rgba(0,255,0,0); }
      100% { box-shadow:0 0 0 0 rgba(0,255,0,0); }
    }
    #statusDot { width:10px; height:10px; border-radius:50%; background:#2ecc71; display:inline-block; margin-right:8px; vertical-align:middle; }
    #transcript { margin-top:12px; min-height:30px; font-size:1.1rem; opacity:.95; }
  </style>
</head>
<body>

  <h1>ðŸŽ¤ FreeFlow Voice Assistant</h1>

  <!-- ma mieÄ‡ ID 'micBtn' (uÅ‚atwia podpiÄ™cie) -->
  <button id="micBtn" aria-label="MÃ³w">ðŸŽ¤</button>

  <div>
    <span id="statusDot"></span>
    <span id="transcript">Dotknij, aby mÃ³wiÄ‡â€¦</span>
  </div>

  <!-- Player do base64 MP3 (musi byÄ‡ PRZED zamkniÄ™ciem body) -->
  <audio id="ttsPlayer" preload="auto"></audio>

  <script>
  // === KONFIG ===
  const API_URL = 'https://freeflow-backend-vercel.vercel.app/api/assistant-text';

  // elementy UI
  const micBtn = document.getElementById('micBtn');
  const transcriptBox = document.getElementById('transcript');
  const statusDot = document.getElementById('statusDot');
  const player = document.getElementById('ttsPlayer');

  // status UI
  function setState(s){
    const colors = { idle:'#2ecc71', listen:'#f1c40f', think:'#3498db', err:'#e74c3c' };
    statusDot.style.background = colors[s] || colors.idle;
    micBtn.classList.toggle('pulse', s==='listen');
    micBtn.dataset.state = s;
  }
  function showText(t){ transcriptBox.textContent = t; }

  // TTS
  async function playBase64Mp3(base64){
    try{
      player.src = `data:audio/mpeg;base64,${base64}`;
      await player.play();
      return true;
    }catch(e){
      console.warn('Audio play error', e);
      return false;
    }
  }
  function speakFallback(text, lang='pl-PL'){
    try{
      const u = new SpeechSynthesisUtterance(text);
      u.lang = lang;
      speechSynthesis.cancel();
      speechSynthesis.speak(u);
    }catch(e){ console.warn('speechSynthesis failed', e); }
  }

  // Backend call
  async function sendToAssistant(userText){
    try{
      setState('think');
      const res = await fetch(API_URL, {
        method:'POST',
        headers:{ 'Content-Type':'application/json' },
        body: JSON.stringify({ text: userText })
      });
      const data = await res.json();

      if(!data.success){
        setState('err'); showText('BÅ‚Ä…d serwera'); speakFallback('WystÄ…piÅ‚ bÅ‚Ä…d po stronie serwera.');
        return;
      }

      const answer = data.assistantText || 'Hmmmâ€¦';
      showText(answer);

      if (data.audioBase64){
        const ok = await playBase64Mp3(data.audioBase64);
        if(!ok) speakFallback(answer);
      } else {
        speakFallback(answer);
      }
      setState('idle');
    }catch(err){
      console.error(err);
      setState('err'); showText('Problem z poÅ‚Ä…czeniem'); speakFallback('Nie udaÅ‚o siÄ™ poÅ‚Ä…czyÄ‡ z serwerem.');
    }
  }

  // ASR (Web Speech)
  function startDictation(){
    const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
    if(!SR){ alert('Twoja przeglÄ…darka nie wspiera rozpoznawania mowy. SprÃ³buj Chrome.'); return; }

    const rec = new SR();
    rec.lang = 'pl-PL';
    rec.interimResults = true;
    rec.maxAlternatives = 1;

    rec.onstart = ()=>{ setState('listen'); showText('SÅ‚uchamâ€¦'); };
    rec.onerror = e => { setState('err'); showText('BÅ‚Ä…d rozpoznawania'); console.warn(e); };
    rec.onresult = e => {
      let final = '', interim = '';
      for (let i=e.resultIndex;i<e.results.length;i++){
        const t = e.results[i][0].transcript;
        if (e.results[i].isFinal) final += t + ' ';
        else interim += t;
      }
      showText((final || interim).trim());
    };
    rec.onend = ()=>{
      const userText = transcriptBox.textContent.trim();
      if (userText) sendToAssistant(userText);
      else setState('idle');
    };

    rec.start();
  }

  // klik = gest dla autoplay
  micBtn.addEventListener('click', ()=>{
    try{ speechSynthesis.cancel(); player.pause(); }catch{}
    startDictation();
  });

  setState('idle');
  </script>
</body>
</html>
