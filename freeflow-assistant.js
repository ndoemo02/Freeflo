/* freeflow-assistant.js
 * Minimalny asystent: ASR (mowa->tekst) + NLU (backend) + TTS (mowa z tekstu)
 * Dzia≈Ça mobilnie na Chrome/Edge (Web Speech API). Ma lekki fallback, gdy ASR niedostƒôpny.
 */

const CONFIG = {
  // >>> PODMIE≈É na sw√≥j backend (masz go na Vercel)
  BACKEND_URL: "https://freeflow-backend-vercel.vercel.app",
  // <<< tylko to zmieniasz w razie potrzeby
};

// ---------- Pomocnicze ----------
const $ = (id) => document.getElementById(id);
const $bubble = $("transcript");
const $micBtn = $("micBtn");
const $tts = $("ttsPlayer"); // w HTML istnieje <audio id="ttsPlayer">
let recognizing = false;

// Uproszczone logowanie do bƒÖbla
function setBubble(html) {
  $bubble.innerHTML = html;
}
function setBubbleText(txt) {
  $bubble.textContent = txt;
}

// ≈Åadna prezentacja zam√≥wienia zamiast surowego JSON
function renderOrder(parsed) {
  // oczekiwane pola z Twojego NLU:
  // { restaurant_name, items:[{name, qty, without?}], when, note?, keep_data? }
  if (!parsed || !parsed.items) {
    setBubble("ü§î <b>Nie uda≈Ço siƒô rozpoznaƒá zam√≥wienia.</b>");
    return;
  }
  const itemsList = parsed.items
    .map(
      (i) =>
        `<li>${i.qty || 1} √ó <b>${i.name}</b>${
          i.without && i.without.length
            ? ` <small>(bez: ${i.without.join(", ")})</small>`
            : ""
        }</li>`
    )
    .join("");

  const when = parsed.when ? parsed.when : "jak najszybciej";
  const rest = parsed.restaurant_name ? parsed.restaurant_name : "‚Äî";

  setBubble(`
    <div style="line-height:1.35">
      <div>üü¢ <b>Zam√≥wienie przyjƒôte</b></div>
      <div>Restauracja: <b>${rest}</b></div>
      <ul style="margin:8px 0 4px 18px">${itemsList}</ul>
      <div>Czas: <b>${when}</b></div>
      ${parsed.note ? `<div>Notatka: ${parsed.note}</div>` : ""}
    </div>
  `);
}

// M√≥w odpowied≈∫ (proste TTS przeglƒÖdarkowe)
function speak(text) {
  try {
    if ("speechSynthesis" in window) {
      const u = new SpeechSynthesisUtterance(text);
      window.speechSynthesis.cancel();
      window.speechSynthesis.speak(u);
    } else {
      // Gdyby≈õ wola≈Ç backendowe /api/tts ‚Äì tu mo≈ºesz zawo≈Çaƒá i odtworzyƒá w <audio>
      // zostawiam proste przeglƒÖdarkowe na teraz
    }
  } catch {}
}

// ---------- HEALTH CHECK ----------
async function checkHealth() {
  try {
    const r = await fetch(`${CONFIG.BACKEND_URL}/api/health`, {
      cache: "no-store",
    });
    const j = await r.json();
    if (j && j.status === "ok") {
      setBubble(`‚úÖ Backend: ok`);
      return true;
    }
  } catch (e) {
    // ignore
  }
  setBubble("‚ö†Ô∏è Backend niedostƒôpny");
  return false;
}

// ---------- NLU ----------
async function sendToNLU(text) {
  const url = `${CONFIG.BACKEND_URL}/api/nlu`;
  const body = { text };

  try {
    const r = await fetch(url, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify(body),
    });

    if (!r.ok) throw new Error(`NLU HTTP ${r.status}`);
    const j = await r.json();

    // Spodziewany shape: { ok:true, parsed:{...}, raw:"..." }
    if (j.ok && j.parsed) {
      renderOrder(j.parsed);
      // kr√≥tki voice feedback:
      const firstItem = j.parsed.items?.[0]?.name || "zam√≥wienie";
      speak(`OK. ${firstItem}. Wysy≈Çam do restauracji.`);
    } else {
      setBubble(
        `ü§î Nie zrozumia≈Çem. <small>${j.error || "spr√≥buj powiedzieƒá inaczej"}</small>`
      );
      speak("Nie zrozumia≈Çem. Spr√≥buj jeszcze raz.");
    }
  } catch (err) {
    setBubble(
      `‚ùå B≈ÇƒÖd NLU. <small>${(err && err.message) || String(err)}</small>`
    );
  }
}

// Udostƒôpniam globalnie ‚Äì do klik√≥w z kafelk√≥w
window.sendToAssistant = (text) => {
  if (typeof text !== "string" || !text.trim()) return;
  setBubble(`üß† ${text}`);
  sendToNLU(text);
};

// ---------- ASR (Web Speech API) ----------
let recognition = null;
(function prepareASR() {
  const SR =
    window.SpeechRecognition ||
    window.webkitSpeechRecognition ||
    window.mozSpeechRecognition ||
    window.msSpeechRecognition;
  if (!SR) {
    // Brak natywnego ASR ‚Äì fallback: prompt po klikniƒôciu
    $micBtn.addEventListener("click", () => {
      const text = prompt("Powiedz/napisz zam√≥wienie:");
      if (text && text.trim()) {
        setBubble(`üß† ${text}`);
        sendToNLU(text);
      } else {
        setBubble("üôÇ ASR niedostƒôpny ‚Äì wpisz tekst rƒôcznie.");
      }
    });
    return;
  }

  recognition = new SR();
  recognition.lang = "pl-PL";
  recognition.continuous = false;
  recognition.interimResults = true;

  recognition.onstart = () => {
    recognizing = true;
    setBubble("üé§ S≈Çucham...");
    // wizualny stan mikrofonu (opcjonalnie mo≈ºna dodaƒá klasƒô CSS)
  };

  recognition.onresult = (e) => {
    let interim = "";
    let final = "";
    for (let i = e.resultIndex; i < e.results.length; i++) {
      const chunk = e.results[i][0].transcript;
      if (e.results[i].isFinal) final += chunk;
      else interim += chunk;
    }
    if (interim) setBubbleText("üéôÔ∏è " + interim);
    if (final) {
      setBubble("üß† " + final);
      sendToNLU(final);
    }
  };

  recognition.onerror = (e) => {
    recognizing = false;
    setBubble("üòï ASR b≈ÇƒÖd: " + e.error);
  };

  recognition.onend = () => {
    recognizing = false;
    // nic ‚Äì czekamy na kolejne klikniƒôcie
  };

  // Klik mikrofonu ‚Äì start/stop
  $micBtn.addEventListener("click", () => {
    try {
      if (!recognizing) {
        recognition.start();
      } else {
        recognition.stop();
      }
    } catch (e) {
      // je≈õli "not-allowed" itp.
      setBubble("‚ö†Ô∏è Nie mam dostƒôpu do mikrofonu.");
    }
  });
})();

// ---------- Szybkie akcje (kafelki) mogƒÖ zawo≈Çaƒá window.sendToAssistant ----------
document.querySelectorAll("[data-quick]").forEach((btn) => {
  btn.addEventListener("click", () => {
    const t = btn.dataset.quick;
    window.sendToAssistant(`Zam√≥wienie: ${t}. Pom√≥≈º doko≈Ñczyƒá szczeg√≥≈Çy.`);
  });
});

// ---------- Start ----------
(async () => {
  await checkHealth();
})();
